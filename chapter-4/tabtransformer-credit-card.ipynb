{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:48.799187Z",
     "end_time": "2023-04-09T12:28:48.800699Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:48.802210Z",
     "end_time": "2023-04-09T12:28:50.632021Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cc-fraud/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:50.633027Z",
     "end_time": "2023-04-09T12:28:50.870448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            Time        V1        V2        V3        V4        V5        V6  \\\n96419    65760.0 -1.234665  0.804234  0.623907 -1.541686  0.517618 -0.205567   \n53817    46159.0 -0.601198 -0.568666  2.048295  0.940042 -0.838668  0.841241   \n220109  142029.0 -1.079029  0.394553  0.983123  0.886915 -0.532981  0.608702   \n162216  114938.0  0.338518  0.864410  0.506710  0.971164  0.489854 -0.512320   \n162222  114940.0 -0.706897  1.438612 -0.273680  2.031884  4.239727  4.180976   \n28559    35053.0  1.045957  0.014432  0.374282  1.249220  0.022520  0.600027   \n279720  169053.0  2.312026 -1.430082 -1.208911 -1.619857 -1.075136 -0.531039   \n242816  151670.0  1.863345  0.203137  0.048925  3.740346 -0.024943  0.766791   \n112720   72776.0  1.120144 -0.043233  0.719266  0.995361 -0.747237 -0.753670   \n21859    31898.0 -0.524524  0.372001 -0.312927 -1.122526 -2.253793  1.222536   \n\n              V7        V8        V9  ...       V21       V22       V23  \\\n96419   1.044546 -0.332273  0.546563  ... -0.235062 -0.132539 -0.225426   \n53817   0.333479  0.002445 -1.883368  ... -0.019566  0.221398  0.338976   \n220109  0.469422  0.581807 -0.683126  ...  0.482845  0.946737  0.126328   \n162216  0.964169 -0.486845 -0.512023  ...  0.346631  1.357021 -0.207703   \n162222  1.444147  0.314025 -2.493999  ...  0.016436  0.115246 -0.604038   \n28559  -0.131237  0.299514  0.114868  ... -0.018569  0.117538 -0.051174   \n279720 -1.106151 -0.138716 -1.059177  ... -0.201595 -0.163631  0.232024   \n242816 -0.578425  0.221620 -0.236275  ...  0.159962  0.504248  0.188202   \n112720  0.006921 -0.197221  0.428150  ... -0.028726  0.120239 -0.098999   \n21859   1.625274 -1.080441  1.068322  ...  0.652621  0.073774  0.270566   \n\n             V24       V25       V26       V27       V28  Amount  Class  \n96419  -0.689264 -0.081429  0.805624  0.036428  0.065066   79.99      0  \n53817  -0.020215  0.178129 -0.035210 -0.159316 -0.224414  221.00      0  \n220109 -0.511093  0.066667 -0.367663  0.007497  0.063784  200.00      0  \n162216  1.226899 -0.486290  0.169571  0.071281 -0.009638   15.62      0  \n162222  0.703100  0.887917  0.317952 -0.592000 -0.378819   25.51      0  \n28559  -0.294268  0.531973 -0.299813  0.045950  0.003886   24.44      0  \n279720  0.425256 -0.149499 -0.175278 -0.005617 -0.048046   25.00      0  \n242816  0.548452 -0.200258  0.004093  0.007896 -0.023884   19.00      0  \n112720  0.804608  0.562700  0.410024 -0.016464  0.023895   49.43      0  \n21859  -0.263758 -1.405662 -0.103877  0.239186  0.113603  403.00      0  \n\n[10 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>96419</th>\n      <td>65760.0</td>\n      <td>-1.234665</td>\n      <td>0.804234</td>\n      <td>0.623907</td>\n      <td>-1.541686</td>\n      <td>0.517618</td>\n      <td>-0.205567</td>\n      <td>1.044546</td>\n      <td>-0.332273</td>\n      <td>0.546563</td>\n      <td>...</td>\n      <td>-0.235062</td>\n      <td>-0.132539</td>\n      <td>-0.225426</td>\n      <td>-0.689264</td>\n      <td>-0.081429</td>\n      <td>0.805624</td>\n      <td>0.036428</td>\n      <td>0.065066</td>\n      <td>79.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>53817</th>\n      <td>46159.0</td>\n      <td>-0.601198</td>\n      <td>-0.568666</td>\n      <td>2.048295</td>\n      <td>0.940042</td>\n      <td>-0.838668</td>\n      <td>0.841241</td>\n      <td>0.333479</td>\n      <td>0.002445</td>\n      <td>-1.883368</td>\n      <td>...</td>\n      <td>-0.019566</td>\n      <td>0.221398</td>\n      <td>0.338976</td>\n      <td>-0.020215</td>\n      <td>0.178129</td>\n      <td>-0.035210</td>\n      <td>-0.159316</td>\n      <td>-0.224414</td>\n      <td>221.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>220109</th>\n      <td>142029.0</td>\n      <td>-1.079029</td>\n      <td>0.394553</td>\n      <td>0.983123</td>\n      <td>0.886915</td>\n      <td>-0.532981</td>\n      <td>0.608702</td>\n      <td>0.469422</td>\n      <td>0.581807</td>\n      <td>-0.683126</td>\n      <td>...</td>\n      <td>0.482845</td>\n      <td>0.946737</td>\n      <td>0.126328</td>\n      <td>-0.511093</td>\n      <td>0.066667</td>\n      <td>-0.367663</td>\n      <td>0.007497</td>\n      <td>0.063784</td>\n      <td>200.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>162216</th>\n      <td>114938.0</td>\n      <td>0.338518</td>\n      <td>0.864410</td>\n      <td>0.506710</td>\n      <td>0.971164</td>\n      <td>0.489854</td>\n      <td>-0.512320</td>\n      <td>0.964169</td>\n      <td>-0.486845</td>\n      <td>-0.512023</td>\n      <td>...</td>\n      <td>0.346631</td>\n      <td>1.357021</td>\n      <td>-0.207703</td>\n      <td>1.226899</td>\n      <td>-0.486290</td>\n      <td>0.169571</td>\n      <td>0.071281</td>\n      <td>-0.009638</td>\n      <td>15.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>162222</th>\n      <td>114940.0</td>\n      <td>-0.706897</td>\n      <td>1.438612</td>\n      <td>-0.273680</td>\n      <td>2.031884</td>\n      <td>4.239727</td>\n      <td>4.180976</td>\n      <td>1.444147</td>\n      <td>0.314025</td>\n      <td>-2.493999</td>\n      <td>...</td>\n      <td>0.016436</td>\n      <td>0.115246</td>\n      <td>-0.604038</td>\n      <td>0.703100</td>\n      <td>0.887917</td>\n      <td>0.317952</td>\n      <td>-0.592000</td>\n      <td>-0.378819</td>\n      <td>25.51</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28559</th>\n      <td>35053.0</td>\n      <td>1.045957</td>\n      <td>0.014432</td>\n      <td>0.374282</td>\n      <td>1.249220</td>\n      <td>0.022520</td>\n      <td>0.600027</td>\n      <td>-0.131237</td>\n      <td>0.299514</td>\n      <td>0.114868</td>\n      <td>...</td>\n      <td>-0.018569</td>\n      <td>0.117538</td>\n      <td>-0.051174</td>\n      <td>-0.294268</td>\n      <td>0.531973</td>\n      <td>-0.299813</td>\n      <td>0.045950</td>\n      <td>0.003886</td>\n      <td>24.44</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>279720</th>\n      <td>169053.0</td>\n      <td>2.312026</td>\n      <td>-1.430082</td>\n      <td>-1.208911</td>\n      <td>-1.619857</td>\n      <td>-1.075136</td>\n      <td>-0.531039</td>\n      <td>-1.106151</td>\n      <td>-0.138716</td>\n      <td>-1.059177</td>\n      <td>...</td>\n      <td>-0.201595</td>\n      <td>-0.163631</td>\n      <td>0.232024</td>\n      <td>0.425256</td>\n      <td>-0.149499</td>\n      <td>-0.175278</td>\n      <td>-0.005617</td>\n      <td>-0.048046</td>\n      <td>25.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>242816</th>\n      <td>151670.0</td>\n      <td>1.863345</td>\n      <td>0.203137</td>\n      <td>0.048925</td>\n      <td>3.740346</td>\n      <td>-0.024943</td>\n      <td>0.766791</td>\n      <td>-0.578425</td>\n      <td>0.221620</td>\n      <td>-0.236275</td>\n      <td>...</td>\n      <td>0.159962</td>\n      <td>0.504248</td>\n      <td>0.188202</td>\n      <td>0.548452</td>\n      <td>-0.200258</td>\n      <td>0.004093</td>\n      <td>0.007896</td>\n      <td>-0.023884</td>\n      <td>19.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>112720</th>\n      <td>72776.0</td>\n      <td>1.120144</td>\n      <td>-0.043233</td>\n      <td>0.719266</td>\n      <td>0.995361</td>\n      <td>-0.747237</td>\n      <td>-0.753670</td>\n      <td>0.006921</td>\n      <td>-0.197221</td>\n      <td>0.428150</td>\n      <td>...</td>\n      <td>-0.028726</td>\n      <td>0.120239</td>\n      <td>-0.098999</td>\n      <td>0.804608</td>\n      <td>0.562700</td>\n      <td>0.410024</td>\n      <td>-0.016464</td>\n      <td>0.023895</td>\n      <td>49.43</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21859</th>\n      <td>31898.0</td>\n      <td>-0.524524</td>\n      <td>0.372001</td>\n      <td>-0.312927</td>\n      <td>-1.122526</td>\n      <td>-2.253793</td>\n      <td>1.222536</td>\n      <td>1.625274</td>\n      <td>-1.080441</td>\n      <td>1.068322</td>\n      <td>...</td>\n      <td>0.652621</td>\n      <td>0.073774</td>\n      <td>0.270566</td>\n      <td>-0.263758</td>\n      <td>-1.405662</td>\n      <td>-0.103877</td>\n      <td>0.239186</td>\n      <td>0.113603</td>\n      <td>403.00</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 31 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:50.869444Z",
     "end_time": "2023-04-09T12:28:50.870953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples in training data: 492 (0.17% of total)\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(df[\"Class\"])\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(df[\"Class\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:50.870953Z",
     "end_time": "2023-04-09T12:28:50.875921Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Class\"] = df[\"Class\"].map({\n",
    "    0: \"Non-fraud\",\n",
    "    1: \"Fraud\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:50.875921Z",
     "end_time": "2023-04-09T12:28:50.888579Z"
    }
   },
   "outputs": [],
   "source": [
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:50.889079Z",
     "end_time": "2023-04-09T12:28:50.890078Z"
    }
   },
   "outputs": [],
   "source": [
    "NUMERIC_FEATURE_NAMES = sorted(filter(lambda v: v != 'Class', list(df.columns.values)))\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {}\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES else [\"NA\"]\n",
    "    for feature_name in df.columns.values\n",
    "]\n",
    "TARGET_FEATURE_NAME = \"Class\"\n",
    "TARGET_LABELS = [\"Non-fraud\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:50.889079Z",
     "end_time": "2023-04-09T12:28:50.891587Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00007\n",
    "WEIGHT_DECAY = 0.0000178\n",
    "DROPOUT_RATE = 0.058\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 3\n",
    "NUM_HEADS = 4\n",
    "EMBEDDING_DIMS = 16\n",
    "MLP_HIDDEN_UNITS = [\n",
    "    256, 256\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:50.891587Z",
     "end_time": "2023-04-09T12:28:50.892593Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/numpy/core/numeric.py:2463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "    return features, target_index\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=list(df.columns.values),\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:50.892593Z",
     "end_time": "2023-04-09T12:28:50.893101Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:50.893101Z",
     "end_time": "2023-04-09T12:28:51.104283Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, embedding_dims):\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            lookup = layers.StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\",\n",
    "            )\n",
    "            encoded_feature = lookup(inputs[feature_name])\n",
    "\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "            )\n",
    "            encoded_categorical_feature = embedding(encoded_feature)\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "        else:\n",
    "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:51.104283Z",
     "end_time": "2023-04-09T12:28:51.106272Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:51.108270Z",
     "end_time": "2023-04-09T12:28:51.108270Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "        embedding_dims,\n",
    "        mlp_hidden_units,\n",
    "        dropout_rate,\n",
    "):\n",
    "    inputs = create_model_inputs()\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    features = layers.concatenate([numerical_features])\n",
    "\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def recall_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    precision = precision_metric(y_true, y_pred)\n",
    "    recall = recall_metric(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:51.108777Z",
     "end_time": "2023-04-09T12:28:51.109775Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def fit_model(\n",
    "        model,\n",
    "        train_data_file,\n",
    "        test_data_file,\n",
    "        num_epochs,\n",
    "        learning_rate,\n",
    "        weight_decay,\n",
    "        batch_size,\n",
    "):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "                 f1_metric,\n",
    "                 precision_metric,\n",
    "                 recall_metric],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(\n",
    "        train_data_file, batch_size, shuffle=True\n",
    "    )\n",
    "    validation_dataset = get_dataset_from_csv(\n",
    "        test_data_file, batch_size\n",
    "    )\n",
    "\n",
    "    callback = keras.callbacks.EarlyStopping(\n",
    "        monitor='loss', patience=3\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset, class_weight=class_weight,\n",
    "        callbacks=[callback], verbose=0\n",
    "    )\n",
    "\n",
    "    _, accuracy, f1, precision, recall = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Validation F1: {f1}\")\n",
    "\n",
    "    return f1, accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:51.108777Z",
     "end_time": "2023-04-09T12:28:51.110775Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 74165\n"
     ]
    }
   ],
   "source": [
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units=MLP_HIDDEN_UNITS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:51.111776Z",
     "end_time": "2023-04-09T12:28:51.113282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 91.84%\n",
      "Validation F1: 0.026406388729810715\n",
      "Validation accuracy: 95.47%\n",
      "Validation F1: 0.053651679307222366\n",
      "Validation accuracy: 89.24%\n",
      "Validation F1: 0.033678654581308365\n",
      "Validation accuracy: 92.72%\n",
      "Validation F1: 0.06240801140666008\n",
      "Validation accuracy: 97.58%\n",
      "Validation F1: 0.0628075897693634\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "acc_scores = []\n",
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "start = time.time()\n",
    "X = df.drop(columns=[\"Class\"], axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "for fold, (train_data_idx, test_data_idx) in enumerate(k_fold.split(X, y)):\n",
    "    train_data_file = f\"cc-fraud/train_data_{fold}.csv\"\n",
    "    test_data_file = f\"cc-fraud/test_data_{fold}.csv\"\n",
    "\n",
    "    train_data = df.iloc[train_data_idx]\n",
    "    test_data = df.iloc[test_data_idx]\n",
    "\n",
    "    train_data.to_csv(train_data_file, index=False, header=False)\n",
    "    test_data.to_csv(test_data_file, index=False, header=False)\n",
    "\n",
    "    f1, accuracy = fit_model(\n",
    "        model=tabtransformer_model,\n",
    "        train_data_file=train_data_file,\n",
    "        test_data_file=test_data_file,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    f1_scores.append(f1)\n",
    "    acc_scores.append(accuracy)\n",
    "end = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:28:51.112276Z",
     "end_time": "2023-04-09T12:34:17.658931Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1-score: 0.04779046475887298\n",
      "Mean accuracy: 0.9336989402770997\n",
      "Training time: 326.7684164047241s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean F1-score: {sum(f1_scores) / len(f1_scores)}\")\n",
    "print(f\"Mean accuracy: {sum(acc_scores) / len(acc_scores)}\")\n",
    "print(f\"Training time: {end - start}s\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T12:34:18.134567Z",
     "end_time": "2023-04-09T12:34:18.137659Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
