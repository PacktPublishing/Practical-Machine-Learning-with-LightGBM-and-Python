{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:06.981604Z",
     "end_time": "2023-04-04T16:43:06.981604Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:06.986639Z",
     "end_time": "2023-04-04T16:43:16.711639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 15)\n",
      "Test dataset shape: (16282, 15)\n"
     ]
    }
   ],
   "source": [
    "HEADERS = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]\n",
    "\n",
    "train_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")\n",
    "train_data = pd.read_csv(train_data_url, header=None, names=HEADERS)\n",
    "\n",
    "test_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    ")\n",
    "test_data = pd.read_csv(test_data_url, header=None, names=HEADERS)\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:16.732958Z",
     "end_time": "2023-04-04T16:43:16.966930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       age          workclass  fnlwgt     education  education_num  \\\n26926   33            Private  312881     Bachelors             13   \n10266   56       Self-emp-inc  184598           9th              5   \n24742   24          State-gov  197731       HS-grad              9   \n16560   24            Private  196332       HS-grad              9   \n29642   45       Self-emp-inc  204196     Bachelors             13   \n25827   40          Local-gov  105862     Bachelors             13   \n5870    51            Private  320513       7th-8th              4   \n18818   25            Private   52536    Assoc-acdm             12   \n13825   54   Self-emp-not-inc  205066          10th              6   \n3164    41            Private  173938   Prof-school             15   \n\n               marital_status         occupation    relationship    race  \\\n26926      Married-civ-spouse     Prof-specialty         Husband   Black   \n10266      Married-civ-spouse   Transport-moving         Husband   White   \n24742      Married-civ-spouse   Transport-moving         Husband   White   \n16560           Never-married      Other-service       Own-child   Black   \n29642                Divorced    Exec-managerial       Unmarried   White   \n25827                Divorced     Prof-specialty       Unmarried   White   \n5870    Married-spouse-absent       Craft-repair   Not-in-family   Black   \n18818                Divorced       Tech-support       Own-child   White   \n13825      Married-civ-spouse       Craft-repair         Husband   White   \n3164       Married-civ-spouse       Adm-clerical         Husband   White   \n\n        gender  capital_gain  capital_loss  hours_per_week  \\\n26926     Male             0             0              40   \n10266     Male             0             0              99   \n24742     Male             0             0              49   \n16560   Female             0             0              40   \n29642     Male             0             0              50   \n25827   Female          5455             0              40   \n5870      Male             0             0              50   \n18818   Female             0          1594              25   \n13825     Male             0             0              36   \n3164      Male             0             0              50   \n\n            native_country income_bracket  \n26926        United-States           >50K  \n10266        United-States          <=50K  \n24742        United-States           >50K  \n16560        United-States          <=50K  \n29642        United-States           >50K  \n25827        United-States          <=50K  \n5870    Dominican-Republic          <=50K  \n18818        United-States          <=50K  \n13825        United-States          <=50K  \n3164                     ?           >50K  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education_num</th>\n      <th>marital_status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>gender</th>\n      <th>capital_gain</th>\n      <th>capital_loss</th>\n      <th>hours_per_week</th>\n      <th>native_country</th>\n      <th>income_bracket</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26926</th>\n      <td>33</td>\n      <td>Private</td>\n      <td>312881</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>10266</th>\n      <td>56</td>\n      <td>Self-emp-inc</td>\n      <td>184598</td>\n      <td>9th</td>\n      <td>5</td>\n      <td>Married-civ-spouse</td>\n      <td>Transport-moving</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>99</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>24742</th>\n      <td>24</td>\n      <td>State-gov</td>\n      <td>197731</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Married-civ-spouse</td>\n      <td>Transport-moving</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>49</td>\n      <td>United-States</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>16560</th>\n      <td>24</td>\n      <td>Private</td>\n      <td>196332</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Never-married</td>\n      <td>Other-service</td>\n      <td>Own-child</td>\n      <td>Black</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>29642</th>\n      <td>45</td>\n      <td>Self-emp-inc</td>\n      <td>204196</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Divorced</td>\n      <td>Exec-managerial</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n      <td>United-States</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>25827</th>\n      <td>40</td>\n      <td>Local-gov</td>\n      <td>105862</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Divorced</td>\n      <td>Prof-specialty</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>5455</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>5870</th>\n      <td>51</td>\n      <td>Private</td>\n      <td>320513</td>\n      <td>7th-8th</td>\n      <td>4</td>\n      <td>Married-spouse-absent</td>\n      <td>Craft-repair</td>\n      <td>Not-in-family</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n      <td>Dominican-Republic</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>18818</th>\n      <td>25</td>\n      <td>Private</td>\n      <td>52536</td>\n      <td>Assoc-acdm</td>\n      <td>12</td>\n      <td>Divorced</td>\n      <td>Tech-support</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>1594</td>\n      <td>25</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>13825</th>\n      <td>54</td>\n      <td>Self-emp-not-inc</td>\n      <td>205066</td>\n      <td>10th</td>\n      <td>6</td>\n      <td>Married-civ-spouse</td>\n      <td>Craft-repair</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3164</th>\n      <td>41</td>\n      <td>Private</td>\n      <td>173938</td>\n      <td>Prof-school</td>\n      <td>15</td>\n      <td>Married-civ-spouse</td>\n      <td>Adm-clerical</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n      <td>?</td>\n      <td>&gt;50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:16.964919Z",
     "end_time": "2023-04-04T16:43:17.001185Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = test_data[1:]\n",
    "test_data.income_bracket = test_data.income_bracket.apply(\n",
    "    lambda value: value.replace(\".\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:16.988051Z",
     "end_time": "2023-04-04T16:43:17.185064Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_file = \"adult/train_data.csv\"\n",
    "test_data_file = \"adult/test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "test_data.to_csv(test_data_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_data_description = train_data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:17.168284Z",
     "end_time": "2023-04-04T16:43:17.195128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:17.212173Z",
     "end_time": "2023-04-04T16:43:17.241748Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_none_last(xs):\n",
    "    return sorted(xs, key=lambda x: (x is None, x))\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"workclass\": sort_none_last(list(train_data[\"workclass\"].unique())),\n",
    "    \"education\": sort_none_last(list(train_data[\"education\"].unique())),\n",
    "    \"marital_status\": sort_none_last(list(train_data[\"marital_status\"].unique())),\n",
    "    \"occupation\": sort_none_last(list(train_data[\"occupation\"].unique())),\n",
    "    \"relationship\": sort_none_last(list(train_data[\"relationship\"].unique())),\n",
    "    \"race\": sort_none_last(list(train_data[\"race\"].unique())),\n",
    "    \"gender\": sort_none_last(list(train_data[\"gender\"].unique())),\n",
    "    \"native_country\": sort_none_last(list(train_data[\"native_country\"].unique()))\n",
    "}\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"fnlwgt\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "COLUMN_DEFAULTS = [\n",
    "    train_data_description[feature_name][\"mean\"] if feature_name in NUMERIC_FEATURE_NAMES else [\"NA\"]\n",
    "    for feature_name in HEADERS\n",
    "]\n",
    "TARGET_FEATURE_NAME = \"income_bracket\"\n",
    "TARGET_LABELS = [\" <=50K\", \" >50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "counts = train_data[\"income_bracket\"].value_counts()\n",
    "class_weight = {\n",
    "    0: (1 / counts[0]) * (train_data.shape[0] / 2.0),\n",
    "    1: (1 / counts[1]) * (train_data.shape[0] / 2.0)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:17.245766Z",
     "end_time": "2023-04-04T16:43:17.262339Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 0.6585962783171521, 1: 2.0763295498023213}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:17.253273Z",
     "end_time": "2023-04-04T16:43:17.263348Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:17.261837Z",
     "end_time": "2023-04-04T16:43:17.294548Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0006\n",
    "WEIGHT_DECAY = 0.00002\n",
    "DROPOUT_RATE = 0.012\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 4\n",
    "NUM_HEADS = 4\n",
    "EMBEDDING_DIMS = 16\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    5,\n",
    "    3,\n",
    "    1,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:17.274908Z",
     "end_time": "2023-04-04T16:43:18.731585Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/numpy/core/numeric.py:2463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "def create_instance(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "    return features, target_index\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=HEADERS,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(create_instance, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:18.734667Z",
     "end_time": "2023-04-04T16:43:18.740674Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:18.745804Z",
     "end_time": "2023-04-04T16:43:18.991330Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, embedding_dims):\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            lookup = layers.StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\",\n",
    "            )\n",
    "            encoded_feature = lookup(inputs[feature_name])\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "            )\n",
    "            encoded_categorical_feature = embedding(encoded_feature)\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "        else:\n",
    "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:18.991828Z",
     "end_time": "2023-04-04T16:43:18.992829Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:18.991828Z",
     "end_time": "2023-04-04T16:43:18.992829Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "\n",
    "    inputs = create_model_inputs()\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def recall_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    precision = precision_metric(y_true, y_pred)\n",
    "    recall = recall_metric(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:18.992829Z",
     "end_time": "2023-04-04T16:43:18.994871Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def fit_model(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "                 f1_metric,\n",
    "                 precision_metric,\n",
    "                 recall_metric],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(\n",
    "        train_data_file, batch_size, shuffle=True\n",
    "    )\n",
    "    validation_dataset = get_dataset_from_csv(\n",
    "        test_data_file, batch_size\n",
    "    )\n",
    "\n",
    "    callback = keras.callbacks.EarlyStopping(\n",
    "        monitor='loss', patience=3\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=validation_dataset,\n",
    "        class_weight=class_weight,\n",
    "        callbacks=[callback]\n",
    "    )\n",
    "\n",
    "    _, accuracy, f1, precision, recall = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Validation F1: {f1}\")\n",
    "\n",
    "    return history"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:18.994341Z",
     "end_time": "2023-04-04T16:43:18.994871Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 435197\n"
     ]
    }
   ],
   "source": [
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:18.994341Z",
     "end_time": "2023-04-04T16:43:19.966564Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:43:19.974634Z",
     "end_time": "2023-04-04T16:45:13.618697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "123/123 [==============================] - 19s 63ms/step - loss: 0.4634 - accuracy: 0.7689 - f1_metric: 0.6303 - precision_metric: 0.5174 - recall_metric: 0.8177 - val_loss: 0.4578 - val_accuracy: 0.7581 - val_f1_metric: 0.6236 - val_precision_metric: 0.4927 - val_recall_metric: 0.8535\n",
      "Epoch 2/15\n",
      "123/123 [==============================] - 6s 48ms/step - loss: 0.4217 - accuracy: 0.7830 - f1_metric: 0.6480 - precision_metric: 0.5337 - recall_metric: 0.8334 - val_loss: 0.4452 - val_accuracy: 0.7659 - val_f1_metric: 0.6176 - val_precision_metric: 0.5024 - val_recall_metric: 0.8053\n",
      "Epoch 3/15\n",
      "123/123 [==============================] - 5s 44ms/step - loss: 0.4148 - accuracy: 0.7854 - f1_metric: 0.6522 - precision_metric: 0.5364 - recall_metric: 0.8396 - val_loss: 0.4888 - val_accuracy: 0.7363 - val_f1_metric: 0.6172 - val_precision_metric: 0.4694 - val_recall_metric: 0.9055\n",
      "Epoch 4/15\n",
      "123/123 [==============================] - 5s 45ms/step - loss: 0.4083 - accuracy: 0.7874 - f1_metric: 0.6550 - precision_metric: 0.5386 - recall_metric: 0.8429 - val_loss: 0.5515 - val_accuracy: 0.7138 - val_f1_metric: 0.6043 - val_precision_metric: 0.4489 - val_recall_metric: 0.9294\n",
      "Epoch 5/15\n",
      "123/123 [==============================] - 6s 45ms/step - loss: 0.4027 - accuracy: 0.7884 - f1_metric: 0.6563 - precision_metric: 0.5407 - recall_metric: 0.8423 - val_loss: 0.4782 - val_accuracy: 0.7520 - val_f1_metric: 0.6260 - val_precision_metric: 0.4860 - val_recall_metric: 0.8832\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - 6s 46ms/step - loss: 0.3996 - accuracy: 0.7901 - f1_metric: 0.6601 - precision_metric: 0.5427 - recall_metric: 0.8498 - val_loss: 0.4489 - val_accuracy: 0.7828 - val_f1_metric: 0.6418 - val_precision_metric: 0.5248 - val_recall_metric: 0.8293\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - 6s 45ms/step - loss: 0.3958 - accuracy: 0.7922 - f1_metric: 0.6630 - precision_metric: 0.5455 - recall_metric: 0.8523 - val_loss: 0.4439 - val_accuracy: 0.7853 - val_f1_metric: 0.6366 - val_precision_metric: 0.5307 - val_recall_metric: 0.7989\n",
      "Epoch 8/15\n",
      "123/123 [==============================] - 6s 48ms/step - loss: 0.3927 - accuracy: 0.7930 - f1_metric: 0.6648 - precision_metric: 0.5464 - recall_metric: 0.8554 - val_loss: 0.4678 - val_accuracy: 0.7657 - val_f1_metric: 0.6293 - val_precision_metric: 0.5026 - val_recall_metric: 0.8452\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - 6s 45ms/step - loss: 0.3905 - accuracy: 0.7939 - f1_metric: 0.6656 - precision_metric: 0.5476 - recall_metric: 0.8550 - val_loss: 0.4948 - val_accuracy: 0.7541 - val_f1_metric: 0.6177 - val_precision_metric: 0.4877 - val_recall_metric: 0.8456\n",
      "Epoch 10/15\n",
      "123/123 [==============================] - 5s 44ms/step - loss: 0.3862 - accuracy: 0.7969 - f1_metric: 0.6699 - precision_metric: 0.5515 - recall_metric: 0.8592 - val_loss: 0.5313 - val_accuracy: 0.7381 - val_f1_metric: 0.6075 - val_precision_metric: 0.4700 - val_recall_metric: 0.8622\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - 5s 43ms/step - loss: 0.3851 - accuracy: 0.7951 - f1_metric: 0.6674 - precision_metric: 0.5488 - recall_metric: 0.8575 - val_loss: 0.4274 - val_accuracy: 0.7926 - val_f1_metric: 0.6374 - val_precision_metric: 0.5427 - val_recall_metric: 0.7756\n",
      "Epoch 12/15\n",
      "123/123 [==============================] - 6s 48ms/step - loss: 0.3859 - accuracy: 0.7958 - f1_metric: 0.6683 - precision_metric: 0.5498 - recall_metric: 0.8583 - val_loss: 0.4507 - val_accuracy: 0.7758 - val_f1_metric: 0.6377 - val_precision_metric: 0.5152 - val_recall_metric: 0.8403\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - 6s 46ms/step - loss: 0.3846 - accuracy: 0.7972 - f1_metric: 0.6704 - precision_metric: 0.5521 - recall_metric: 0.8596 - val_loss: 0.4608 - val_accuracy: 0.7586 - val_f1_metric: 0.6305 - val_precision_metric: 0.4935 - val_recall_metric: 0.8768\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - 6s 45ms/step - loss: 0.3819 - accuracy: 0.7970 - f1_metric: 0.6709 - precision_metric: 0.5517 - recall_metric: 0.8619 - val_loss: 0.4164 - val_accuracy: 0.7821 - val_f1_metric: 0.6457 - val_precision_metric: 0.5237 - val_recall_metric: 0.8461\n",
      "Epoch 15/15\n",
      "123/123 [==============================] - 6s 46ms/step - loss: 0.3813 - accuracy: 0.7994 - f1_metric: 0.6725 - precision_metric: 0.5551 - recall_metric: 0.8592 - val_loss: 0.4270 - val_accuracy: 0.7700 - val_f1_metric: 0.6386 - val_precision_metric: 0.5072 - val_recall_metric: 0.8657\n",
      "Validation accuracy: 77.0%\n",
      "Validation F1: 0.6385642886161804\n",
      "Training time: 113.62454104423523s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = fit_model(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-04T16:45:13.618697Z",
     "end_time": "2023-04-04T16:45:13.619206Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
